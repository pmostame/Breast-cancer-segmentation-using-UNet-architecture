{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Here, I set to mark areas of the breast ultrasound scans that are cancerous using image segmentation with deep learning. To this goal, I designed a UNet architecture from scratch (in pytorch) which yielded a strong performance measured by Dice index. \n\n**Dice index was around 70% when averaged over test data samples.**\n<br><br>\nThe analysis steps:\n* Import libraries\n* Collect data path for images\n* Load a subset of data to estimate mean and std for normalization purposes\n* Use the mean and std to create image transforms/augmentations\n* Separately load data samples into train and test sets\n* Visual inspection of data\n* Design the UNet\n* Instantiate a UNet\n* Train the model\n* Visualize model performance across epochs\n* Save the model\n* Visually inspect some of the segmentations in comparison to the ground truth\n* Check for potential biases in model performance across cancer types\n","metadata":{}},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as f\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n\nfrom PIL import Image\n\nimport torchvision\nimport torchvision.datasets as datasets\nimport torchvision.transforms as T\nfrom torchvision.io import read_image\n\nimport re\n\ndata_path = '/kaggle/input/breast-ultrasound-images-dataset/Dataset_BUSI_with_GT'\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:54:04.414608Z","iopub.execute_input":"2023-08-11T15:54:04.415088Z","iopub.status.idle":"2023-08-11T15:54:07.740409Z","shell.execute_reply.started":"2023-08-11T15:54:04.415054Z","shell.execute_reply":"2023-08-11T15:54:07.738192Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Collect image paths","metadata":{}},{"cell_type":"code","source":"image_list = []\n\nfolders = os.listdir(data_path)\nfor folder in folders:\n    if folder != 'normal':\n        files = os.listdir( os.path.join(data_path, folder) )\n        for file in files:\n            if 'mask' not in file:\n                image_list.append( os.path.join(data_path, folder, file) )\n            \nrandom.shuffle(image_list)\n\ndata_length = len( image_list )\n\nimage_list_train = image_list[:np.floor(0.9*data_length).astype(int)]\nimage_list_test = image_list[np.floor(0.9*data_length).astype(int):]\n\nprint( f'Total length of data: {data_length}')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T15:54:08.559285Z","iopub.execute_input":"2023-08-11T15:54:08.560041Z","iopub.status.idle":"2023-08-11T15:54:08.886841Z","shell.execute_reply.started":"2023-08-11T15:54:08.560013Z","shell.execute_reply":"2023-08-11T15:54:08.885349Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# define hyperparameters","metadata":{}},{"cell_type":"code","source":"BATCH_SIZE = 16\nIMAGE_SIZE = 64\nCHANNEL_LIST = [1, 16, 32, 64]\n\nN_EPOCHS = 100\n\nclass_to_inx = {'benign': 0, 'malignant': 1}\ninx_to_class = {0: 'benign', 1: 'malignant'}","metadata":{"execution":{"iopub.status.busy":"2023-08-11T18:49:26.509713Z","iopub.execute_input":"2023-08-11T18:49:26.510309Z","iopub.status.idle":"2023-08-11T18:49:26.519183Z","shell.execute_reply.started":"2023-08-11T18:49:26.510272Z","shell.execute_reply":"2023-08-11T18:49:26.517514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Data class","metadata":{}},{"cell_type":"code","source":"class BreastData(Dataset):\n    def __init__(self, image_list, data_mode='Train', transform=None):\n        self.image_list = image_list\n        self.data_mode = data_mode\n        self.image_transform = transform[0]\n        self.mask_transform = transform[1]\n        \n    def __len__(self):\n        return len(self.image_list)\n    \n    def extract_cancer_type(self, image_path):\n        if 'benign' in image_path:\n            cancer_type = 0\n        elif 'malignant' in image_path:\n            cancer_type = 1\n        return cancer_type\n    \n    def __getitem__(self, index):\n        image_path = self.image_list[index]\n        image = Image.open(image_path).convert('RGB')\n        \n        mask_path = image_path.split('.')[0] + '_mask.png'\n        mask = Image.open(mask_path).convert('1')\n        \n        image = self.image_transform(image)\n        mask = self.mask_transform(mask)\n            \n        cancer_type = self.extract_cancer_type(image_path)    \n        return image, mask, cancer_type","metadata":{"execution":{"iopub.status.busy":"2023-08-11T17:39:28.449936Z","iopub.execute_input":"2023-08-11T17:39:28.450424Z","iopub.status.idle":"2023-08-11T17:39:28.461051Z","shell.execute_reply.started":"2023-08-11T17:39:28.450390Z","shell.execute_reply":"2023-08-11T17:39:28.459754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define Tversky loss as a performance metric","metadata":{}},{"cell_type":"code","source":"class tverskyloss(nn.Module):\n    def __init__(self, alpha=0.5, beta=0.5, reduction='mean'):\n        super().__init__()\n        self.alpha = alpha\n        self.beta = beta\n        self.reduction = reduction\n        \n    def forward(self, pred, target):\n        loss = 1 - tversky_index(pred, target, alpha=self.alpha, beta=self.beta, reduction=self.reduction)\n        return loss\n\ndef tversky_index(y_pred, y_true, alpha=0.5, beta=0.5, reduction='mean'):\n    # generalization of dice coefficient algorithm\n    #   alpha corresponds to emphasis on False Positives\n    #   beta corresponds to emphasis on False Negatives (our focus)\n    #   if alpha = beta = 0.5, then same as dice\n    #   if alpha = beta = 1.0, then same as IoU/Jaccard\n    smooth = 1e-5\n    \n    if y_true.ndim > 3:\n        y_true_f = y_true.reshape(y_true.shape[0], -1)\n        y_pred_f = y_pred.reshape(y_pred.shape[0], -1)\n    else:\n        y_true_f = y_true.flatten()\n        y_pred_f = y_pred.flatten()\n        \n    intersection = torch.sum(y_true_f * y_pred_f)\n    tversky = (intersection + smooth) / ( intersection + alpha * (torch.sum(y_pred_f*(1 - y_true_f))) + beta *  (torch.sum((1 - y_pred_f)*y_true_f)) + smooth)\n    \n    if reduction == 'mean':\n        tversky = tversky.mean()\n    return tversky","metadata":{"execution":{"iopub.status.busy":"2023-08-11T17:39:29.618883Z","iopub.execute_input":"2023-08-11T17:39:29.620124Z","iopub.status.idle":"2023-08-11T17:39:29.629941Z","shell.execute_reply.started":"2023-08-11T17:39:29.620088Z","shell.execute_reply":"2023-08-11T17:39:29.628484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define image transforms/augmentations","metadata":{}},{"cell_type":"code","source":"image_transforms = {}\n\nimage_transforms['mask'] = T.Compose([\n    T.Resize( int(np.floor(IMAGE_SIZE * 1.05)) ),\n    T.ToTensor(),\n    T.Grayscale(),\n    T.CenterCrop(IMAGE_SIZE)\n])","metadata":{"execution":{"iopub.status.busy":"2023-08-11T18:49:31.206720Z","iopub.execute_input":"2023-08-11T18:49:31.207106Z","iopub.status.idle":"2023-08-11T18:49:31.215048Z","shell.execute_reply.started":"2023-08-11T18:49:31.207078Z","shell.execute_reply":"2023-08-11T18:49:31.212981Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# find mean and std for normalization purposes\ntransform = [image_transforms['mask'], image_transforms['mask']]\nimage_dataset = BreastData(image_list_train, transform=transform)\nimage_dataloader = torch.utils.data.DataLoader(image_dataset, batch_size=100, shuffle=True)\ndata_length = len(image_dataset)\n\nsubset = next(iter(image_dataloader))[0].permute(1, 0, 2, 3).reshape(1, -1)\n\nmean = subset.mean(-1)\nstd = subset.std(-1)\nprint(f'Mean: {mean} --- std: {std}')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T18:49:33.682047Z","iopub.execute_input":"2023-08-11T18:49:33.683216Z","iopub.status.idle":"2023-08-11T18:49:34.932716Z","shell.execute_reply.started":"2023-08-11T18:49:33.683174Z","shell.execute_reply":"2023-08-11T18:49:34.931295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_transforms['train'] = T.Compose([\n    T.Resize( int(np.floor(IMAGE_SIZE * 1.05)) ),\n    T.ToTensor(),\n    T.Grayscale(),\n    T.CenterCrop(IMAGE_SIZE),\n    T.Normalize(mean=mean, std=std),\n    T.RandomErasing(p=0.1, scale=(0.01, 0.1), ratio=(0.2, 5)),\n])\n\nimage_transforms['test'] = T.Compose([\n    T.Resize( int(np.floor(IMAGE_SIZE * 1.05)) ),\n    T.ToTensor(),\n    T.Grayscale(),\n    T.CenterCrop(IMAGE_SIZE),\n    T.Normalize(mean=mean, std=std),\n])","metadata":{"execution":{"iopub.status.busy":"2023-08-11T18:49:39.028136Z","iopub.execute_input":"2023-08-11T18:49:39.028679Z","iopub.status.idle":"2023-08-11T18:49:39.039879Z","shell.execute_reply.started":"2023-08-11T18:49:39.028639Z","shell.execute_reply":"2023-08-11T18:49:39.036972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Split Image data into Train and Test sets","metadata":{}},{"cell_type":"code","source":"train_transform = [image_transforms['train'], image_transforms['mask']]\ntest_transform = [image_transforms['test'], image_transforms['mask']]\n\ntrain_dataset = BreastData(image_list_train, transform=train_transform)\ntest_dataset = BreastData(image_list_test, transform=test_transform)\n\ntrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\ntest_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)\n\nprint(f'Train set size: {len(train_dataset)} ----- Test set size: {len(test_dataset)}')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T18:49:40.143401Z","iopub.execute_input":"2023-08-11T18:49:40.143845Z","iopub.status.idle":"2023-08-11T18:49:40.154330Z","shell.execute_reply.started":"2023-08-11T18:49:40.143810Z","shell.execute_reply":"2023-08-11T18:49:40.152551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visual inspection of random data samples","metadata":{}},{"cell_type":"code","source":"images, masks, labels = next(iter(train_dataloader))\nlabels = np.array(labels)\n\nfig, ax = plt.subplots(4,8, figsize=(20,12))\nax = ax.ravel()\nfor a in ax:\n    a.axis('off')\n    \nfor ind, (image, mask, label) in enumerate( zip(images, masks, labels) ):\n    if ind < 16:\n        plt.sca(ax[2 * ind])\n        plt.imshow(image.permute(1,2,0), cmap='gray')\n        plt.title(f'#{ind+1}: {inx_to_class[label]}')\n        plt.sca(ax[2 * ind + 1])\n        plt.imshow(mask.permute(1,2,0))\n        plt.title(f'#{ind+1}')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T18:49:41.578051Z","iopub.execute_input":"2023-08-11T18:49:41.578431Z","iopub.status.idle":"2023-08-11T18:49:44.166129Z","shell.execute_reply.started":"2023-08-11T18:49:41.578400Z","shell.execute_reply":"2023-08-11T18:49:44.165047Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# design UNet from scratch","metadata":{}},{"cell_type":"code","source":"class unet(nn.Module):\n    def __init__(self, CHANNEL_LIST):\n        super().__init__()\n        self.encoder_block = encoder(CHANNEL_LIST[:-1]) # 128 channels\n        self.flat_block = conv_block(CHANNEL_LIST[-2], CHANNEL_LIST[-1]) # 256 channels\n        self.decoder_block = decoder(CHANNEL_LIST[::-1][:-1]) # 1 channel\n        self.output_block = conv_block(CHANNEL_LIST[1], 1) # 1 channel\n        \n    def forward(self, x):\n        encoder_outputs, x = self.encoder_block(x)\n        x = self.flat_block(x)\n        x = self.decoder_block(x, encoder_outputs)\n        x = f.sigmoid( self.output_block(x) )\n        return x    \n\nclass conv_block(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, padding='same', bias=False)\n        self.batchnorm1 = nn.BatchNorm2d(out_channels)        \n        self.dropout = nn.Dropout2d(0.02)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, padding='same')\n    \n    def forward(self, x):\n        x = self.dropout( f.relu( self.batchnorm1( self.conv1(x) ) ) )\n        x = self.conv2(x)\n        return x\n    \n    \nclass encoder(nn.Module):\n    def __init__(self, channel_list):\n        super().__init__()\n        self.blocks = nn.ModuleList([\n            conv_block(channel_list[i], channel_list[i+1])\n            for i in range(len(channel_list)-1)\n        ])\n        self.maxpool = nn.MaxPool2d(2, 2)\n    \n    def forward(self, x):\n        encoder_outputs = []\n        for block in self.blocks:\n            x = block(x)\n            encoder_outputs.append(x)\n            x = self.maxpool(x)\n        return encoder_outputs, x\n    \nclass decoder(nn.Module):\n    def __init__(self, channel_list):\n        super().__init__()\n        self.upsamples = nn.ModuleList([\n            nn.ConvTranspose2d(channel_list[i], channel_list[i+1], 2, 2)  \n            for i in range(len(channel_list)-1)\n        ])\n        self.blocks = nn.ModuleList([\n            conv_block(channel_list[i], channel_list[i+1])\n            for i in range(len(channel_list)-1)\n        ])\n    \n    def forward(self, x, encoder_outputs):\n        encoder_outputs_reversed = encoder_outputs[::-1]\n\n        for ind, (upsample, block) in enumerate( zip(self.upsamples, self.blocks) ):            \n            x = upsample(x) # doubles the spatial dimension and decrease channels\n            x = torch.cat([x, encoder_outputs_reversed[ind]], dim=1) # channels are increased again\n            x = block(x) # decrease number of channels\n        return x\n    \ndef initialize_weights(model):\n    for m in model.modules():\n        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d)):\n            nn.init.normal_(m.weight.data, 0, 0.02)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T19:38:32.715729Z","iopub.execute_input":"2023-08-11T19:38:32.716121Z","iopub.status.idle":"2023-08-11T19:38:32.732767Z","shell.execute_reply.started":"2023-08-11T19:38:32.716091Z","shell.execute_reply":"2023-08-11T19:38:32.730637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Instantiate the model","metadata":{}},{"cell_type":"code","source":"UNet = unet(CHANNEL_LIST).to(device)\ninitialize_weights(UNet)\n\ncriterion = tverskyloss(alpha=0.5, beta=0.5)\n\noptimizer = optim.Adam(UNet.parameters(), lr=0.001)\nlr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=1, factor=0.5)","metadata":{"execution":{"iopub.status.busy":"2023-08-11T19:38:34.832015Z","iopub.execute_input":"2023-08-11T19:38:34.833958Z","iopub.status.idle":"2023-08-11T19:38:34.849585Z","shell.execute_reply.started":"2023-08-11T19:38:34.833898Z","shell.execute_reply":"2023-08-11T19:38:34.847526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the model","metadata":{}},{"cell_type":"code","source":"train_losses = []\ntest_losses = []\ntrain_metrics = []\ntest_metrics = []\nlrs = []\n\nfor epoch in range(N_EPOCHS):\n    UNet.train()\n    \n    current_lr = optimizer.state_dict()['param_groups'][0]['lr'] \n    lrs.append( current_lr )\n    \n    running_loss = 0\n    running_metric = 0\n    \n    for (images, masks, _) in train_dataloader:\n        images = images.to(device)\n        masks = masks.to(device)\n        \n        optimizer.zero_grad()\n        output = UNet(images)\n        \n        # loss \n        loss = criterion(output, masks)\n        running_loss += loss.item() * images.shape[0]\n        \n        # accuracy metric\n        img1 = torch.where(output.detach().cpu() >= 0.5, 1, 0)\n        img2 = masks.detach().cpu()\n        running_metric = running_metric + tversky_index(img1, img2) * images.shape[0]\n        \n        loss.backward()\n        optimizer.step()\n        \n    lr_scheduler.step( running_loss )\n\n    train_losses.append( running_loss / len(train_dataset) )\n    train_metrics.append( running_metric / len(train_dataset) * 100 )\n    \n\n    with torch.no_grad():\n        UNet.eval()\n        \n        running_loss = 0\n        running_metric = 0\n        for (images, masks, _) in test_dataloader:\n            images = images.to(device)\n            masks = masks.to(device)\n        \n            output = UNet(images)\n            \n            # loss\n            loss = criterion(output, masks)\n            running_loss += loss.item() * images.shape[0]\n        \n            # accuracy metric\n            img1 = torch.where(output.detach().cpu() >= 0.5, 1, 0)\n            img2 = masks.detach().cpu()\n            running_metric = running_metric + tversky_index(img1, img2) * images.shape[0]\n        \n        test_losses.append( running_loss / len(test_dataset) )\n        test_metrics.append( running_metric / len(test_dataset) * 100 )\n        \n    print(f'[{epoch+1}/{N_EPOCHS}]: train_loss={train_losses[-1]:0.2f} / test_loss={test_losses[-1]:0.2f} -------------  train_tversky={train_metrics[-1]:0.2f} / test_tversky={test_metrics[-1]:0.2f} -------------  lr={current_lr:0.8f}')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T19:38:36.062963Z","iopub.execute_input":"2023-08-11T19:38:36.063925Z","iopub.status.idle":"2023-08-11T19:59:21.587519Z","shell.execute_reply.started":"2023-08-11T19:38:36.063874Z","shell.execute_reply":"2023-08-11T19:59:21.586312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualizer model performance across epochs","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 1, figsize=(8,12))\nax[0].plot(np.array(train_losses), marker='o', label='Train_loss')\nax[0].plot(np.array(test_losses), marker='o', label='Test_loss')\nax[0].legend()\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Losses')\n\nax[1].plot(np.array(train_metrics), marker='o', label='Train_metric')\nax[1].plot(np.array(test_metrics), marker='o', label='Test_metric')\nax[1].legend()\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Dice index')\n\nax[2].plot(np.array(lrs), marker='o', label='Train_accuracy')\nax[2].set_xlabel('Epochs')\nax[2].set_ylabel('Learning rate')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T20:20:56.710398Z","iopub.execute_input":"2023-08-11T20:20:56.710793Z","iopub.status.idle":"2023-08-11T20:20:57.312214Z","shell.execute_reply.started":"2023-08-11T20:20:56.710764Z","shell.execute_reply":"2023-08-11T20:20:57.310883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Save the model","metadata":{}},{"cell_type":"code","source":"torch.save(UNet, '/kaggle/working/trained_model')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T20:21:16.237271Z","iopub.execute_input":"2023-08-11T20:21:16.237682Z","iopub.status.idle":"2023-08-11T20:21:16.251707Z","shell.execute_reply.started":"2023-08-11T20:21:16.237650Z","shell.execute_reply":"2023-08-11T20:21:16.250205Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visually inspect segmentations","metadata":{}},{"cell_type":"code","source":"fig, ax = plt.subplots(3, 10, figsize=(22, 6))\nfor this_ax in ax.ravel():\n    this_ax.axis('off')\n    \nfor counter, ind in enumerate(np.random.choice(np.arange(len(test_dataset)), 10)):\n    image, mask, _ = test_dataset[ind]\n    \n    ax[0, counter].imshow( image.permute(1,2,0) )\n    ax[0, counter].set_title('Breast scan')\n    \n    ax[1, counter].imshow( mask.permute(1,2,0) )\n    ax[1, counter].set_title('Actual area')\n    \n    segmentation = (UNet(torch.unsqueeze(image, 0).to(device)).detach().cpu() > 0.5)\n    ax[2, counter].imshow( torch.squeeze(segmentation, 0).permute(1,2,0) )\n    ax[2, counter].set_title('Predicted are')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T20:23:36.596048Z","iopub.execute_input":"2023-08-11T20:23:36.596544Z","iopub.status.idle":"2023-08-11T20:23:38.734727Z","shell.execute_reply.started":"2023-08-11T20:23:36.596510Z","shell.execute_reply":"2023-08-11T20:23:38.733767Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check for possible biases in model performance per cancer type","metadata":{}},{"cell_type":"code","source":"mask.shape","metadata":{"execution":{"iopub.status.busy":"2023-08-11T20:25:49.953196Z","iopub.execute_input":"2023-08-11T20:25:49.953608Z","iopub.status.idle":"2023-08-11T20:25:49.960716Z","shell.execute_reply.started":"2023-08-11T20:25:49.953569Z","shell.execute_reply":"2023-08-11T20:25:49.959407Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"jaccard_index_per_type = {key: [] for key in class_to_inx.keys()}\n\nfor (image, mask, label) in test_dataset:\n    image = torch.unsqueeze(image, 0)\n    mask = torch.unsqueeze(mask, 0)\n    \n    segmentation = torch.where(UNet(image.to(device)).detach() > 0.5, 1, 0)\n    j = tversky_index(segmentation, mask)\n    \n    key = inx_to_class[label]\n    jaccard_index_per_type[key].append( j )\n\njaccard_index_per_type = {key: np.array(val) for key, val in jaccard_index_per_type.items()}","metadata":{"execution":{"iopub.status.busy":"2023-08-11T20:26:31.630880Z","iopub.execute_input":"2023-08-11T20:26:31.631275Z","iopub.status.idle":"2023-08-11T20:26:32.896501Z","shell.execute_reply.started":"2023-08-11T20:26:31.631246Z","shell.execute_reply":"2023-08-11T20:26:32.895515Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for ind, (key, val) in enumerate(jaccard_index_per_type.items()):\n    X = ind + np.random.randn(val.size) * 0.05\n    plt.scatter(X, val, alpha=0.5)\n    plt.plot([ind-0.2, ind+0.2], [val.mean(), val.mean()], linewidth=5)\n\nplt.xlabel('Cancer type')\nplt.ylabel('Jaccard index')\n\n_ = plt.xticks(ticks=[0, 1], labels=jaccard_index_per_type.keys())\npercentage = jaccard_index_per_type['benign'].mean() / jaccard_index_per_type['malignant'].mean() * 100\n\nif percentage > 100:\n    key_better = 'benign'\nelse:\n    key_better = 'malignant'\npercentage = abs( percentage - 100 )\nprint(f'Model performs better for class \"{key_better}\" by [{percentage:0.2f}] percent in Dice index.')","metadata":{"execution":{"iopub.status.busy":"2023-08-11T20:27:50.171287Z","iopub.execute_input":"2023-08-11T20:27:50.172648Z","iopub.status.idle":"2023-08-11T20:27:50.364564Z","shell.execute_reply.started":"2023-08-11T20:27:50.172541Z","shell.execute_reply":"2023-08-11T20:27:50.362629Z"},"trusted":true},"execution_count":null,"outputs":[]}]}